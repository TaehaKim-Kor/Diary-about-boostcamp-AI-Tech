<details>
<summary>Writing Specification</summary>
<div markdown="1">

>Date : 22.01.24
>
>강좌 분류 : Pytorch
>
>>강좌 번호 : 1
>>
>>제목 : Introduction to Pytorch
>
>>강좌 번호 : 2
>>
>>제목 : Pytorch Basics
>
>>강좌 번호 : 3
>>
>>제목 : Pytorch 프로젝트 구조 이해하기

</div>
</details>

<details>
<summary>오늘 들은 강의 총평</summary>
<div markdown="1">

드디어, 파이토치 강의가 시작되었다.

음.. 배우긴 했는데, 직접한 건 많이 없었다.

Tip 위주로 다루면 Tip을 익히고, 전체적인 작성 규약에 대해 알아보면서 직접 작성해보는 시간을 가져보겠다.

다행인 건 이번 주부터는 강의 내용이 길지는 않아서 듣는데 부담이 없다.

내 공부를 더 하면서 부스트캠프 외 필요한 공부를 더 할 수 있을 것 같다.

</div>
</details>

<details>
<summary>강의 내용</summary>
<div markdown="1">

<details>
<summary>Introduction to Pytorch</summary>
<div markdown="1">

Computational Graph : 연산의 과정을 그래프로 표현

> Define and Run(Static Graph) : 그래프를 먼저 정의함 -> 실행 시점에서 데이터를 feed
> 
> Define by Run(Dynamic Computational Graph, DCG) : 실행을 하면서 그래프를 생성하는 방식

>> 그래프 생성 타입에 따라서 굉장히 많은 차이가 존재함.
>>
>> 1. 동적 그래프 방식은 실행 중 즉시 내부 값을 확인 가능하다.
>>
>> 2. 동적 그래프 방식은 코딩하기 편함(sess.run이 없어.)
>>
>> 3. 정적 그래프 방식은 Production과 Scalability에 장점이 있음.

>>> Production의 의미에는 Cloud 연결, TPU, multi GPU 등을 의미하지만
>>>
>>> 내 경험상 가장 정적 그래프 방식의 강점은 C++의 포팅이 편하다는 점이 있다.
>>>
>>> 동적 그래프 방식은 C++로 포팅할 때 그 그래프 구조를 C++에서 일일이 정해주어야 포팅이 가능했다.
>>>
>>> 그래서 몇몇 라이브러리가 현재 ONNX 모듈로 바꿔주는 함수를 지원하긴 하는데, 대부분이 완벽하지 않다.
>>>
>>> 반대로 말하면, 동적 그래프 방식이 그만큼 능동적인 코딩이 가능하단 의미이기도 하다.

Pytorch = Numpy + AutoGrad(자동미분) + Function(신경망 구성에 필요한 함수를 지원)

</div>
</details>

<details>
<summary>Pytorch Basics</summary>
<div markdown="1">

Tensor in Pytorch = N-dimensional Arrays in  Numpy

넘파이의 ndarray와 생성 방식이 동일하다.(List로 불러오기)
> ndarray로 Tensor를 만드는 것도 가능하다.

그 외 함수들도 거의 비슷하게 지원한다.
> (ndim = 차원 수를 반환, shape = 차원 형태와 각 차원의 element 숫자를 반환)

Tensor의 차원, 형태를 handling하는 대표적인 함수는 아래와 같음.

> 1. view : 입력된 tensor의 shape를 입력된 형태로 변환한다. 출력은 입력된 tensor의 값을 "참조"한다. 
> 
> 2. reshape : 입력된 tensor의 shape를 입력된 형태로 변환한다. 출력은 입력된 tensor의 값을 "복사"한다. 
>> 이를 view와 reshape은 contiguity 보장의 차이가 있다고 말하며, 보통 view를 많이 쓰게 될 것임.
>
> 3. squeeze : 차원의 개수가 1인 차원을 삭제한다.
> 
> 4. unsqueeze : 차원의 개수가 1인 차원을 추가해준다.

Tensor의 연산도 Numpy 행렬의 연산과 거의 같음. 단 행렬 곱셈 연산은 아래와 같은 차이를 보임.

> 1. dot : Numpy와 달리 1-D Tensor, Vector간의 연산만을 지원함.
> 
> 2. mm : 2-D 이상의 행렬간의 곱셈 연산, broadcasting 지원을 안함.(가장 많이 씀.)
> 
> 3. matmul : 2-D 이상의 행렬간의 곱셈 연산, broadcasting 지원을 함. (의도치 않은 결과를 초래할 수 있음.)

그 외 손실함수, softmax, one-hot vector 변환과 같은 다양한 함수를 nn.functional 모듈을 통해 지원하고 있음.

Autograd는 자동 미분을 지원하는 개념으로, backward 함수를 사용하면 미분값을 반환해 줌.

예시를 들어서 설명하려고 하신 것 같은데, 어쨋든 tensor를 생성하여 자동 미분을 활성화하려면,

생성 단계에서 requires_grad를 True로 명시해줘야함.

그러나 nn.linear같은 대부분의 layer함수들은 이 옵션이 default로 되어 있음.

</div>
</details>

<details>
<summary>Pytorch 프로젝트 구조 이해하기</summary>
<div markdown="1">

강의 과정에서 Pytorch Template에 대해서 알게 되었다.

사실 처음부터 작성하려면 굉장히 막막한 감이 있는데 굉장히 도움이 된 느낌이 있다.

주어진 template에서 먼저 확인하는 파일은 train.py이다.

argparse를 이용한 configuration 설정과 데이터 로드 하는 부분, train하는 부분을 집중적으로 봐야함.

jupyter notebook을 벗어나야 하는 이유는 파이토치 프로젝트 구조상,

data_centric AI(대용량 데이터를 처리하는 AI)시대에서 terminal에서 실행시켜야하는 경우가 많기 때문임.

이 때문에 대화형 코딩에서 벗어난 코딩도 굉장히 중요한 요소가 되었음.


</div>
</details>

</div>
</details>

<details>
<summary>후일담 및 차기 계획</summary>
<div markdown="1">

뭔가 한 것이 없는데 너무 졸렸다.

일찍 일어나서 작성했는데, 요즘 이런 삶이 반복되다보니 커피 섭취량이 늘었다.

아메리카노 뽑아먹을 시간이 없어 편의점 커피를 마시는데,

건강에 영향이 있을 여지가 있는 것이 조금 걱정임.(특히 살 찌는 거)

차기 계획

1. Pytorch 4강 5강 듣고 기본과제 풀기
   
2. 코딩 기본과제 풀기

3. 논문 찾아보기
   
</div>
</details>

