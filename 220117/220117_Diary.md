Writing Specification
>Date : 22.01.17
>
>강좌 분류 : boostcamp AI Tech - AI Mathmatics
>
>>강좌 번호 : 1
>>
>>제목 : 벡터가 뭐에요?
>
>>강좌 번호 : 2
>>
>>제목 : 행렬이 뭐에요?
>
>>강좌 번호 : 3
>>
>>제목 : 경사하강법 순한 맛
>
>강좌 분류 : boostcamp AI Tech - Python
>
>>강좌 번호 : 1-1
>>
>>제목 : Basic computer class for newbies
>
>>강좌 번호 : 1-2
>>
>>제목 : 파이썬 개요



**오늘 들은 강의 총평 및 나의 감상**

기본적으로 작성하면서 그냥 따로 공부한 것들을 정리해보았거나 떠들고 싶은 것을 여기 적었다.

강의를 보며 제공된 교재에 공부한 필기 노트는 내가 가지고 있는데, 이걸 public에 공개할 방법이 없어서(네이버와 싸우긴 싫다.)

어떻게 공개 가능하면서 공부도 꽤 보람차고, 실용적이게 할까 조금 고민해보았는데,

별다른 포맷은 안 떠오르는 문제가 발생했다... pptx 대충 손보고 밥먹고 나니 9시가 지나서 강의도 더 못 들었다..

(아니 근데 9시는 너무 짧다.. 11시까지만 해줘도 좋을텐데.. 근데 새벽 2시 30분엔 또 볼 수 있다.)

뭐.. 그래서 오랜만에 수학적인 센스도 살릴 겸, 벡터가 무엇인지 수학적으로 다시 찾는 기회를 가져보기로 했다.

거기에 github로 다이어리를 쓰면서 github 사용 센스도 늘리고...

매번 지켜지지 않는 개인적인 깨달음이지만, 인공 지능 강의를 대학원에서도 듣고 연구에도 써보고 해봤는데,

스킬은 지식 없이도 사용 가능한데, 스킬을 쌓는 것에 그치지 않으려면 수학을 좀 잘 해야한다는 결론이 섰다.

연구실에서도 실제로 KL-Divergence의 수학적인 의미가 무엇이냐 가지고 싸워봤고..

확률 이론을 제대로 이해 못해서 멍청한 소리를 지도 교수님 주관 세미나 앞에서 해본 적도 있고..

(나중에 책을 읽고 제가 잘못 설명드렸다면서 Slack으로 다시 설명드리는 그 처절한 기분...)

오늘은 아무튼 벡터에 대해 자세히 공부해보았다. 다음은 확률론이려나?...

파이썬 강의는 컴퓨터에 관한 것과 파이썬 역사, 언어적 특성과 관련된 것이었는데,

뭐 강의 듣고 느낀 것은 아 파이썬은 **너무너무너무** 쉬운 언어라는 것 정도.. 그래도 잘못 쓰면 디버깅하기 어렵다..

컴퓨터 지식은 넘어가자.

대부분의 하드웨어 전부 내가 끼워 맞춰 보고 소프트웨어적으로 필요한 드라이버 등은 전부 스스로 최적화 해낸다.

> **벡터 - 숫자를 원소로 가지는 리스트 또는 배열.**
> 
> 수학적으로 조금 궁금해서 확인해본 바에 의하면 벡터를 이해하기 전에 벡터 공간을 이해하는 편이 낫다.
> 
> 벡터 공간은 벡터들의 집합으로, 이 벡터들은 서로 더해질 수 있고, 또 스칼라에 의하여 곱해질 수 있음.
> 
> 그리고 벡터는 벡터 공간 내에서 정의되는 원소를 지칭한다.(출처 - [Wikipedia - Vector(mathematics and physics)](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)))
>
>> 그런데 여기 내용을 읽다보니까 벡터라는 단어는 사원수(Quaternion)를 설명하면서 소개되었다는 말이 있는데, 개인적으로 꽤나 흥미로웠다. **사원수 때문에 박살난 내 로봇공학개론 학점 때문은 아니다.**
>
> 교재에 있는 **"벡터는 공간에서의 한 점을 나타냅니다."** 라는 의미는, 결론적으로 벡터 공간에서의 원소 하나를 벡터라고 지칭하는 것이다.(벡터 공간은 그 모든 벡터들을 담은 집합을 의미하는 것이고.)
> 
> 여기 정의대로라면, 애시당초 벡터는 벡터의 특성에 의해서 덧셈과 스칼라곱이 정의되는 것이 아니라, 어떤 공간 내에서 덧셈과 스칼라 곱이 정의되는 원소들을 벡터로 부르기로 한 것이라고 볼 수 있다.
> 
>> 말이 조금 이상한데, 결론적으로 더해지고 스칼라곱이 가능한 배열들을 모아놓고 이것은 벡터라고 하는 것.
>>
>> 수학, 특히 선형대수학 같은 과목을 배우다 보면 Space에 대한 개념을 많이 배웠던 거 같은데 기억이 가물가물하구만... 실수 공간.. 허수 공간...
> 
> 그러니, **"벡터끼리 같은 모양을 가지면 덧셈, 뺄셈을 계산할 수 있다."** 라는 의미는
> 
>> "두 벡터가 같은 공간에 속한다면 덧셈과 뺄셈은 벡터 공간의 정의에 의해 정의할 수 있다"는 이야기라고 보면 될 것 같다.(나아가 스칼라곱까지도!)
>
> 그 다음 이해를 위해서, 우선 벡터 공간에서 특이한 성질을 가진 벡터를 정의할 필요가 있다.
>
>> 영 벡터(Zero Vector) : 어떤 벡터든 영벡터와 더했을 때 그 벡터를 유지하는 벡터 공간 내 원소를 지칭.
>>
>> 기저 벡터(Basis Vector) : 벡터 공간 내 기저(Basis)를 나타내는 벡터, 쉽게 좌표축 위에 있는 벡터를 지칭하지만, 꼭 좌표축일 필요는 없다. 기저 벡터의 조합은 유니크한 것이 아니기 때문임.
>>
>>> 무슨 소리냐면, 쉽게 카테시안 좌표계에서 존재하는 2차원 벡터공간에서 x와 y의 축의 방향을 살짝 회전시켜서 새로운 좌표계를 만들어도 그 좌표계의 좌표축으로 기저 벡터 조합을 결정할 수 있다는 소리이다. 심지어 직교할 필요도 없다.
>>
>> 단위 벡터(Unit Vector) : 벡터 공간 내 벡터의 진폭이 1인 벡터를 지칭
>
> 저 중에서 영 벡터는 다음에 나오는 **"원점으로부터 상대적 위치를 표현한다."** 라는 구절을 설명하는데 도움을 준다.
> 
> 여기서 나오는 원점이 바로 영 벡터를 의미하기 때문이다.
> 
> 정리하자면 벡터가 원점으로부터 상대적 위치를 표현한다는 이유는, 벡터에 영벡터를 더했을 때 그 벡터가 바뀌지 않는 특성을 이용하면([Additive Identity](https://en.wikipedia.org/wiki/Additive_identity))
> 
>> 1) 영 벡터를 빼더라도 그 벡터는 그대로일 것이고, => Vec(x) - Vec(0) = Vec(x)
>>
>> 2) 영 벡터를 뺀 벡터는 원점에서 출발하여 벡터가 있는 위치에 도달하는 벡터를 의미하기 때문이다.
>
> **"벡터에 숫자를 곱해주면 길이만 변합니다."** 라는 말의 의미는 벡터의 구성 요소에 대해 고민할 필요가 있다.
> 
> 통상적으로 벡터는 진폭(Magnitude)와 방향(Direction)로 구성되어 있다.
> 
> 진폭은 벡터의 절대적인 크기를 지칭, 방향은 공간에서의 벡터의 방향을 의미할텐데..
>
> 이를 설명하기 위해서 나는 위에서 대충 넘어간 기저 벡터를 다시 가지고 와야 했다..
> 
> 기저 벡터, 설명을 대충하고 넘어갔는데, 대충 기저 벡터가 되기 위해선 다음의 조건을 만족해야 한다.
>> 1) 기저벡터는 선형 독립이어야 함.(Linear Independent)
>>
>> 2) 벡터 공간 내 벡터를 기저벡터들의 선형 조합(Linear Combination)으로 표현할 수 있음.
>>
>> 출처 - [Basis(Linear Algebra)](https://en.wikipedia.org/wiki/Basis_(linear_algebra))
>
> 사실 이게 뭐가 중요한데..? 라고 생각할 수도 있는데
>
> 방향을 설명하기에 이것만큼 괜찮은 수식이 없기도 한 듯하다.
> 
> 기저 벡터 조건 2번을 보면, 벡터 공간 내 모든 벡터는 기저 벡터들의 선형 조합으로 표현할 수 있다고 되어있는데,
> 
> 쉽게 표현하면 모든 기저벡터들에 각각 알맞은 계수(Coefficient)를 스칼라곱해주어서 더해주면 어떤 벡터든 표현할 수 있다는 이야기이다.
> 
>> 이 스칼라곱은 벡터 공간과 벡터의 정의에 의해서 정의되었으므로 이 설명은 순서에 맞는 듯하다.
>
> 그렇다면 모든 계수들을 이용해 유클리드 거리(L2-Distance)를 계산해서 이를 모든 기저벡터에 스칼라곱으로 나누어주면,
>
> 남는 각 기저벡터들의 계수들로 계산한 유클리드 거리는 1이 되는데, 이때 나눠진 이 벡터를 방향은 같고 크기가 1인 단위 벡터로 볼 수가 있다.
> 
>> 예시로, [2,0] 이라는 벡터가 있으면, (2^2+0^2)^0.5 = 2 이므로 2를 나누어 [1,0]인 벡터를 만들어도 같은 방향을 가리키는 벡터를 볼 수 있음.
>>
>> 다른 예시로, [2,1,3] 이라는 벡터가 있으면, (2^2 + 1^2 + 3^2)^0.5 = sqrt(14)니까 이 수를 나눈 [2/sqrt(14),1/sqrt(14),3/sqrt(14)]를 만들어도 방향은 똑같고 크기는 1인 벡터를 얻을 수 있음.
>
> 그리고 이 과정에서 우리는 이미 벡터의 크기를 줄여보았다.
>
> 반대로 스칼라곱으로 곱해주었으면 벡터의 크기가 늘어났을 것이다.
>
> 그리고 이 과정에서 벡터의 방향은 바뀌지 않았다.
> 
>> 증명을 이런 식으로 하면 안 되는 건 알지만, 어차피 내 다이어리인데 틀리지만 않으면...
>
> 노름(Norm)은 옛날(무려 2년 전)에 머신 러닝 강의 들을 때 배웠다.
> 
> 교재에선 L1, L2 Norm만 배웠는데, n-Norm 나아가 L-$\infin$ Norm도 정의할 수 있다.(심지어 n은 실수면 된다.)
> 
> 특징은 L 뒤의 숫자가 커질수록 2차원 카테시안 좌표계 공간에서 진폭이 1인 벡터들의 집합을 그리면 점점 [1,1] 지점에 그 집합이 그어버린 선이 가까워져 L-$\infin$ Norm까지 가면 완전히 사각형의 형태가 되는 것이다.
> 
> 수식은 아래와 같다.
> 
>> $\|\vec{x}\|_{n} \= ^{1 \over n}\displaystyle\sum_{i=1}^{d}{|x_i|^n}$
>>
>>> 시간 나면 코드로 직접 그려봐야지 했는데, (막 이미지를 가져다 쓰고 싶어져서) 귀찮아져 버렸다.
>>>
>>>거기에 Github 수식입력도 안 되는지 이번에 처음 알았다. Latex가 이럴 땐 짱 좋네 논문 쓸 때는 거지 같았는데..
>
> Lasso, Ridge 회귀나 Robust 학습, Laplace 근사 모두 들어보고 실제로 몇몇은 풀어본 것 같은데 기억이 안 난다.
> 
> 여유로울 때가 있을지 모르겠지만 대학원 교재를 다시 봐야할 듯 하다. 머신 러닝 이론 하면서 이걸 피할 방법이 없었는데... 이걸 다시 보게 되다니...
>
> 행렬, 경사하강법은 벡터 공부하느라 힘을 다 썼고, 대부분 쉬운 내용들이라서 패스했다. Affine Transformation 같은 내용이 나오면 조금 다뤄봐야지.