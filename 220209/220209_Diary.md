<details>
<summary>Writing Specification</summary>
<div markdown="1">

>Date : 22.02.09
>
>강좌 분류 : DL Basic
>
>>강좌 번호 : 7
>>
>>제목 : Sequential Models - RNN
>
>>강좌 번호 :  8
>>
>>제목 : Sequential Models - Transformer


</div>
</details>

<details>
<summary>오늘 들은 강의 총평</summary>
<div markdown="1">

Transformer.. 참 어려운 친구다.

그래도 강의를 수없이 되돌리며 듣고 이해해본 결과,

Q,K,V로 무엇을 나타내고 싶은건지 정도는 표현할 수 있을 것 같다.

또, 철학적인 관점에서의 Transformer의 구조를 먼저 조금 생각해보게 된다.

다만 논문을 제대로 보면 완벽할 것 같긴 하다만..

일단 오늘은 이해한 수준까지만 정리하는 걸로.


</div>
</details>

<details>
<summary>강의 내용</summary>
<div markdown="1">

<details>
<summary>Sequential Models - RNN</summary>
<div markdown="1">

시스템에 들어올 입력으로의 시계열 데이터는 length와 interval을 완벽히 예측할 수 없는 문제점이 있다.
> interval은 이산 시스템에서 sampling period를 조절하여 예측하긴 해도 결손이 생길 수도 있으니까

그래서 데이터의 length를 제한한 Autoregressive Model이 제안되었다.
> 대표적인 Autoregressive model로 Markov Model이 있다.(1차 자기회귀 모델)
> 
> Markov Decision Process를 따르는 가정(Markov Assumption)을 깔면,
> 
> 이전의 데이터와 현재 데이터만으로 모델링할 수 있고, 이는 매우 간결한 조건부 확률식 표현으로 나타난다.

데이터의 Length를 제한했다는 한계점을 극복하기 위해 Latent Autoregressive Model, RNN이 제안되었다.

> Latent는 이전의 모든 데이터를 요약한 어떤 특징이라고 생각하면 된다.

RNN 이야기는 이전에 지겹게 했으니 pass하고(IIR이 무엇인지 기억난다면 당신은 에이스다.)

LSTM(Long Short Term Memory)은 RNN의 Long-term dependencies에서 취약점을 극복하기 위해 제안되었다.

Forget / Input / Output gate라고 불리우는 것들과 Cell-State/Hidden State를 나누어 정보를 처리한다.

1. Forget Gate : 이전 스텝의 Cell state의 정보 혼합 비율을 현재 스텝의 Input과 이전 스텝의 Hidden state로 계산하여 조절하는 게이트.
   
2. Input Gate : 이전 스텝의 Cell state의 정보 혼합 비율을 현재 스텝의 Cell state candidate와 이전 스텝의 Hidden State로 계산하여 조절하는 게이트.
   
(Cell State candidate는 비슷한 수식으로 Hyperbolic Tangent로 계산되었음.)

3. Output Gate : 이전 스텝의 Hidden State와 Input으로 정보 혼합 비율을 계산하여 현재 스텝의 Cell State를 얼마나 보낼지 조절하는 게이트

GRU는 게이트를 Reset gate와 Update gate로 재편하여 구성하고 Cell state와 Hidden state를 합친 개념으로, 정보 혼합비율을 각각 조절하는 것이 아닌, bilinear intepolation 개념으로 조절하는 것.

</div>
</details>

<details>
<summary>Sequential Models - Transformer</summary>
<div markdown="1">

내가 생각하는 RNN과 Transformer의 차이점

RNN은 매번 들어오는 데이터와 이전의 latent를 활용해 현재 스텝에서의 출력을 계산한다.
> Transformer는 들어올 수 있는 모든 데이터를 Embedding하고, Embedding된 vector들의 Implicit/Explicit한 특징들을 type별로 뽑아 보관 후, Positional Encoding을 통해 Sequence를 구현
>> 마치, 우주(Universe Set, 전체 집합)에서 발생할 수 있는 모든 입력 값을 미리 보관해두고
>>> Embedding 과정이 이런 것임.
>>
>> 그 입력값들 간의 관계를 추론할 수 있는 다양한 feature들을 미리 생성하여
>>> Attention(Query/Key)으로 표현되는 것임.
>>
>> 들어온 데이터의 Order와 Embedding된 Vector를 활용하여 파악한 우주 내 모든 입력에서 가장 관련있는 벡터를 반환함.
>>> Value/Positional Encoding를 사용하여 Refinement되는 과정으로 표현되는 것임.

RNN 역시 학습 과정에서 일부 데이터에 대해서는 Embedding이 미리 필요하고, Embedding되지 않은 값에 대해서는 출력이 정확할 것이라고 장담하진 못하지만,

Transformer는 애시당초 Embedding된 입력이 아니면 해석할 수 없는 구조를 가진다.
(학습 과정에서 입력되지 않은 Embedding된 Vector를 위한 공간을 더 늘려줄 수는 있겠지만, 이게 잘 학습되지는 않을 것임.)

Attention에서 핵심 요소를 담당하는 4개는 나는 이렇게 생각한다.

1. Query : Embedding된 Vector의 Implicit한 Feature

2. Key : Embedding된 Vector와 자기자신을 포함한 다른 Vector간의 관계를 나타내는 Explicit한 Feature

3. Value : Embedding된 Vector의 Attention을 경유했을 때의 자기자신을 포함한 다른 Vector간의 관계를 나타내는 Explicit한 Feature

4. Positional Encoding : Query, Key, Value로 계산된 각 Embedding된 Vector간의 관계성에 Sequence 관계를 추가해주는 Feature

나머지도 정리해두었는데, 이건 논문을 읽고 보정할 계획이다.

~~이 위도 바뀔수도 있다.~~

</div>
</details>


</div>
</details>

<details>
<summary>후일담 및 차기 계획</summary>
<div markdown="1">

Transforemr.. 감은 잡히는데 완벽하지 않다.

어차피 다음 논문 스터디가 Transformer이다보니

논문과 같이 읽으면서 정리할 계획이다.

어제 Generative Model을 공부했어야했지만, 못했기때문에 오늘 해야한다.

차기 계획

1. 9~10강 듣기
   
2. ViT / AAE 논문 읽기
   
3. Attention 논문 읽기(여유시간에)

</div>
</details>

