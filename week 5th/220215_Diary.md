<details>
<summary>Writing Specification</summary>
<div markdown="1">

>Date : 22.02.15
>
>강좌 분류 : AI 서비스 개발 기초
>
>>강좌 번호 : 1
>>
>>제목 : 강의 소개
>
>>강좌 번호 : 2
>>
>>제목 : 머신러닝 프로젝트 라이프 사이클
>
>>강좌 번호 : 3
>>
>>제목 : Linux & Shell Command

</div>
</details>

<details>
<summary>오늘 들은 강의 총평</summary>
<div markdown="1">

AI 서비스를 개발하면서 어떤 마음가짐을 해야하는지 강조하는 강의였다고 생각한다.

사실 자연스러운 경험담에 가깝지 않았나 싶기도 한데, 하다보면 몸으로 체득할만한 것들을 미리 알게 된 기분이었다.

Linux 강의는... 사실 나는 그냥 뭔가 필요하면 stack overflow같은 곳에 많이 참조하는 편이라

그게 진짜 중요한 명령어였는지 몰랐었는데.. 데이터 전처리도 가능한거 보니까 해볼만한 것 같기도 하다.

오늘은 linux를 본체에 키고 노트북으로 강의를 들으며 해볼 생각이다.

이유는 윈도우가... 날아가면 안 되기 때문에 ㅎㅎ

</div>
</details>

<details>
<summary>강의 내용</summary>
<div markdown="1">

강의 내용이 기본적으로 너무 많아 이중으로 작업하기 어렵고,

스페셜 미션의 경우에는 이미지와 코드를 첨부해 제대로 정리하면 좋을 것 같아서,

Github에는 리스트를 작성해두고 Notion에 옮겨서 정리하려고 한다.

<details>
<summary>머신러닝 프로젝트 라이프 사이클</summary>
<div markdown="1">

>Special Mission
>>1. 부스트캠프 AI Tech 또는 개인 프로젝트를 앞선 방식으로 정리해보기

</div>
</details>

<details>
<summary> Linux & Shell Command</summary>
<div markdown="1">

>Special Mission
>>1. test.txt 파일에 "Hi!!!!!!"를 입력하기(vi 사용 금지)
>>
>>2. test.txt 파일 맨 아래에 "kkkkkk"를 입력하기(vi 사용 금지)
>>
>>3. test.txt의 라인 수를 구하기(wc -l을 쓰면 구할 수 있음.)
>>> 여기까지의 모든 문제는 한 줄의 명령어로 구현할 것
>>
>>4. 학습한 쉘 커맨드 정리하기
>>
>>5. 카카오톡 그룹 채팅방에서 옵션-대화 내보내기로 csv 저장 후, 쉘 커맨드 1줄로 카카오톡 대화방에서 2021년에 제일 메세지를 많이 보낸 top 3명을 1줄로 추출하기

</div>
</details>

<details>
<summary>Transformer - 멘토링 내용</summary>
<div markdown="1">

>오늘 멘토님께 드릴 질문
>>Transformer에서 Query Key Value의 의미(Key와 Value의 차이)
>>
>> Transformer 디코더에서 Key와 Value를 왜 인코더의 출력에서 가져오는가
>>
>> Positional Encoding에서 왜 any fixed offset k에 대해서 Positional Encoding 함수가 선형 함수로 표현할 수 있는가?
>>
>>Positional Encoding을 입력으로 더해주는 것이 어떻게 위치를 기억하는 방법이 되는지(Positional Encoding의 동작 원리)

1번 대답

> 현실에서, 데이터베이스 기준
>
> Query : 내가 검색한 단어의 특성
>
> Key : 데이터베이스에서 검색된 데이터의 특성
>
> value : 그 값.
>
>> self attention에서 QKV는 일단 다 자기자신에서 나옴.

내적 : 비슷한 것을 보는 수단으로 활용할 수 있음.

이는 cosine 유사도를 크기로 반영하는 것임.

Query와 Key의 내적을 계산한 score는 그러면 유사도가 되는 것이다.

Value는 output의 형태의 원형이 되는 것임.

2번 대답

Query는 검색하는 것이고, K와 V는 학습된 맥락에서 가져오는 것이기 때문.

encoder에서 abstract시킨 정보 key, value를 바탕으로 query에서 가장 유사한 값을 찾아낸다.

Key와 Value는 학습된 데이터베이스로 생각

3번 대답

Embedding vector에 없는 위치 정보를 반영해야 한다.

RNN같은 자기회귀모델은 순서대로 입력을 주기때문에 그 순서정보가 이미 반영되어있고, 그로인해 Gradient Vanishing/Exploding 문제가 발생함.

그러면 1,2,3,4처럼 순차대로 넣으면 안되겠냐?

-> embedding과정이 조금 난해해짐(dimension 충돌문제)

-> 만분의1~만분의만까지 넣어놓으면 되지않겠냐

-> test셋이 embedding된 값이 더 많아지면 어떻게할건데? -> 결국 숫자를 그대로 때려박는 건 좋지 않다.

-> 실제로 concat부터 모든 것을 다 해봤다.

일단 embedding은 1대1 함수여야한다.

여러 특성 중에서, 1번 대답에 해당하는것은

1. 숫자가 매우 커질수있어서 훈련때보다 더 큰 값이 들어오면 안된다.

2. 일반화가 가능해져야한다.

3. 서로다른 두 타임스텝간의 거리는 항상 일정해져야한다.

    -> 거리가 k인 단어들 사이에서는 같은 관계를 가져야하기 때문에

4. key값처럼 결정해야한다.

삼각함수식에서 t번째와 t+k번째를 넣어도 그 변환관계가 회전변환이 되고, 이 거리는 일정하기 때문에 선형적이라고 보는 것이다.

결국 positional encoding은 위의 4가지를 고려할 수 잇는 회전 변환(각좌표계)을 나타내는 것이다.

4번 대답

embedding vector와 positional encoding vector를 더하는 이유는 일단 곱하는거에 비해서 실험적으로 더 잘 되었던 것이다.

그리고 positional encoding은 어떻게보면 입력에 t를 더 더해주는 것이라고 보면 된다. 

이 t에 대해서도 gradient flow가 흐르기때문에 위치정보를 보정하는 개념이 된다고 생각하면 된다.

time t에 대한 것도 (positional encoding같이) embedding시켜서 한다.

(diffusion도 이런 계열에 해당함.)

두 입력의 크기는 서로 비슷하게 embedding한다.

</div>
</details>


</div>
</details>

<details>
<summary>후일담 및 차기 계획</summary>
<div markdown="1">

Transformer 멘토링이 너무 기억에 남는다. 꽤 유익했었음.

그 외에는.. zep 이거 아주 못 쓸 물건이다.

어제 하다가 너무 스트레스를 받았다. ㅋㅋ. 뭐 저래

이것이 메타버스라면.. 후.. 미래가 참 암울하다.

더 나아지겠지 ㅠㅠ...제발...

새삼 그런 일들을 겪으면서 온라인게임 서버같은 것들이 얼마나 대단하고 어렵게 설계되는 것들인지

알게되었다.

Zoom에 움직이는 캐릭터를 추가했을 뿐인데 저런다니.. 믿겨지지 않는다.

최대 접속 가능 인원도 30~40명 수준인것같던데 보니까...

~~대체 로스트아크 서버는 어떻게 구현되어 있는거지~~

차기 계획

1. AI 서비스 개발 개론 4~5강 듣기

2. Transformer 최종 마무리

</div>
</details>

