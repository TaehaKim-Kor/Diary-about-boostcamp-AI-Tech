<details>
<summary>Writing Specification</summary>
<div markdown="1">

>Date : 22.01.27
>
>강좌 분류 : Pytorch
>
>>강좌 번호 : 8
>>
>>제목 : Multi-GPU 학습
>
>>강좌 번호 : 9
>>
>>제목 : Hyperparameter Tuning
>
>>강좌 번호 : 10
>>
>>제목 : Pytorch Troubleshooting

</div>
</details>

<details>
<summary>오늘 들은 강의 총평</summary>
<div markdown="1">

오늘 강의는 뭔가 대학원에서 겪었던 짜증나는 moment들을 모아 놓은 느낌이 강하다.



</div>
</details>

<details>
<summary>강의 내용</summary>
<div markdown="1">

<details>
<summary>Multi-GPU 학습</summary>
<div markdown="1">

Multi-GPU 학습을 하면서 시간에 쫓겨 사는 우리들에게 **돈으로 시간을 살 수 있다는** 것을 알게 해준다.

Deadline에 쫓길 때, 전기와 CPU와 GPU가 무한히 많다면, 수많은 세팅을 한 번에 돌리며 실험을 진행한다.
> 뭐... 전기가 무한한 건 아니지만, 전기료는 학교가 내고(난 등록금을 내지만)
>
> CPU... 많으면 좋겠지만 이 친구가 많다고 일이 비례해서 편해지는 것은 아니지만
>
> GPU는 많으면 그냥 편하다... 리얼..
>
> CUDA_VISIBLE_DEVICE에 0보다 많이 찍히는 컴퓨터를 쓰고 있으면 정말 황홀하기 그지없다.

다수의 GPU를 활용하는 방식에도 교재에 나와있듯 모델을 분산하는지와 데이터를 분산하는지에 차이가 있다.

이상적인 케이스에 대해서는 당연하게도 엄청 똑똑한 GPU 하나가 엄청 큰 저장공간을 활용해 학습했으면 좋겠지만,

거대한 모델을 학습하고 싶지만 하드웨어적으로 그럴 수 없는 현실을 타파하기 위한 방법이라고 생각하면 좋다.

AlexNet이 모델을 분산한 대표적인 CNN 모델로, 당시에 연구자들이 학습에 활용한 GPU가 GTX 580(3GB VRAM)이다.
> VRAM 3GB가 있는 GTX 580... 지금 딥러닝 용으로 출시되는 쿼드로 모델이 최소 24GB라는 것을 생각해보면..

구조를 자세히 보면, CONV3/FC6~8 이렇게 4개의 Layer에서 두 GPU간 통신을 하고 나머지는 GPU 내에서 전파된다.

이런 식의 모델 분산화는 조금 구현하기도 어렵고 문제점들이 있다.

> GPU 내 코어들간의 버스를 통한 통신속도와 메인보드 PCI Express 버스를 통한 통신속도의 차이로 병목 발생
>
> 학습 파이프라인이 복잡해짐. 동기화 문제이기도 하고.

데이터 병렬 처리는 모델 병렬 처리에 비하면 너무 쉽고 간단하다.

연산의 주체만 다를 뿐, Mini-batch와 똑같기 때문이다.

연산의 주체를 정하는 방법을 포함해, Multi GPU를 구현하는 방법은 2가지다.

> 1. DataParallel : nn.moudle에 씌워주는 것으로, GPU를 분산처리하게 함.
> 
> 단, 특정 GPU가 데이터를 한번에 처리하는 과정이 존재하는데, 이로 인해서 
> 
> 특정 GPU의 처리 능력이 제한되고, 그에 맞춰 모든 GPU의 전체적인 분산처리 성능이 감소함.
> 
> 2. DistributedDataParallel : Dataset과 Dataloader에 적용하는 것으로, CPU도 분산처리를 위해 할당됨.
> 
> 기본 연산은 DataParallel과 같으나, 개별적으로 자기 연산의 평균을 구하여 특정 GPU에 부하를 몰아주지 않음.
> 
> 선언한 Dataset에 torch.utils.data.distributed.DistributedSample를 사용해 Sampler를 선언하고
> 
> Dataloader를 선언할 때 sampler에 선언한 것을, num_worker를 min(GPU*3~4, CPU의 총 Thread)만큼 넣어준다.

</div>
</details>

<details>
<summary>Hyperparameter Tuning</summary>
<div markdown="1">

Hyperparameter Tuning... 난 별로 좋아하지 않는다.

이게 진짜 요즘 들어서는 정말 최후의 순간까지 몰려서 해보는 방법인지라

(거기에 드는 연산에 비해서 효과도 별로 없는 것 같다. 그래도 하긴 해야함..)

Data를 더 신경써서 보는 것이 낫다는 교수님 강의에는 굉장히 동의한다.

>그리고 모델(또는 모델의 구조)를 크게 바꾸는 것은 연구 토픽과 반대로 가는 경우라서 적용하기 어렵기도 하고

조금 이야기해보고 싶었던 것은 [NAS(Neural Architecture Search)](https://arxiv.org/abs/1611.01578)를 이용해 Hyperparameter Tuning하는 이야기가 나왔는데,

무려.. 강화 학습 이론으로 Hyperparameter를 결정하는 방법이다. Block을 어떻게 쌓을 것인지도 결정하는..

아마.. 주말 쯤에 Notion을 정리하면서 이 내용을 간략하게 요약해볼 계획이다.

이렇게 강화학습 기반이 아니면 주로 쓰이는 방법은
> 1. Grid search
> 
> 2. Random search
> 
> 3. Bayesian search

등이 있고, Ray라는 라이브러리를 활용해서 이를 쉽게 해보는 것이라고 배웠는데,

수작업으로 일일이 구현해서 파악하는 것 보단 낫겠더라.

</div>
</details>

<details>
<summary>Pytorch Troubleshooting</summary>
<div markdown="1">

Troubleshooting 정말 중요한 문제다.

아... 이 수만가지 오류들... 어떻게 에러메세지를 만들었을까? 라는 생각이 들만큼.

Multi-GPU 학습 과정에서 4개의 GPU에 학습을 돌려놓고 나오면 1개가 꼭 죽어있다거나..하는 일이 있긴 하다.

특히 0번 GPU가 그럴 일이 있는데,

1. 모니터를 연결해두면 모니터 작동을 위한 GPU 점유때문에 다른 GPU보다 일을 더 많이 하기도 하고,

2. 또 다른 GPU에서 얻어 온 정보를 종합하느라 다른 GPU보다 일을 더 많이 하기도 하는 등

하나로 특정할 수 없는 이유들 때문에 저런 일들이 발생하곤 한다.

그래서 메모리 점유를 잘 확인해보면서 해야 같은 것을 두 번 학습하지 않는다.

메모리 문제는 또 다른 공학적인 측면에서도 중요한데,

실제로 인공지능과 함께 뭔가 만들려고 할 때 메모리 누수를 해결하지 못하면,

그 메모리 누수로 인해 내가 짠 신경망이 아닌 다른 프로그램이 갑자기 먼저 죽어버리기도 한다.

메모리 문제를 해결하기 위해서 강의에선 아래의 함수나 방법론을 제시한다.

1. with torch.no_grad() : backward pass를 위해 저장하는 정보를 없애는 context

2. torch.cuda.empty_cache() : 사용되지 않는 GPU Cache를 정리해주는 함수. Del과는 쓰임이 다르니 유의

3. training loop에서 tensor로 축적되는 변수는 확인 : tensor 변수는 GPU에 남기 때문에 확인 후 처리
> 1-d tensor의 경우엔 python 기본 객체(tensor.item / float(tensor))로 처리한다.
> 
> 필요가 없어진 변수는 del을 이용해 적절히 삭제한다.(loop 이후에도 메모리를 점유하기 때문)

4. batch size 줄이기 : nvidia-smi 켜라. 우분투 터미널에서 보고 싶으면 아래의 명령어도 추천한다.

> watch -d -n 0.5 nvidia-smi

5. 정밀도 줄이기 : 32비트 정밀도로 처리되는 변수를 16비트로 처리하면 연산에 필요한 메모리가 감소한다.

</div>
</details>


</div>
</details>

<details>
<summary>후일담 및 차기 계획</summary>
<div markdown="1">

요즘 아침 6시에 일어나서 10시까지 공부한 것을 정리 중이다.

그런데 문제가 생겼다. Colab을 자꾸 쓰다보니 제일 중요한 오후 시간대에 GPU 할당이 중지되어 버린다다.

.... 난 Local에서 돌릴 자원도 있는데 이런 불상사가...

python 파일로 바꿔서 Ubuntu에서 돌리면 어떨까 싶었는데

Ubuntu에서 부스트캠프 활동하려니 바꿀 것들이 한가득이라 여간 귀찮은 일이 아니다.

아.. 근데 신경망 공부하는 것에는 Ubuntu가 압도적으로 편하긴 한데.. 이 것도 고민이 좀 된다.

오늘 일이 있어서 정말 오랜만에 연구실에 들렸는데, 연구실 친구들이 내 다이어리를 보고 있었다고 했다.

물론 바쁜 시간 할애해서 읽어주는 건 감사한 것이다. 그런데 모두가 읽으려면 영어로 작성해야 하나...?

사실 처음 다이어리를 쓴 날, 그러니까 1월 17일 다이어리의 초안은 사실 영어로 작성되어 있었다.

그런데 영어 실력도 많이 모자라기도 하고, 그래서 작성에 너무 오랜 시간이 걸렸고

내가 공부한 교재들에 나의 필기도 전부 영어로 적혀 있어서 한글로 푼 자료가 필요해 한글로 옮긴 것이었다.

오.. 근데 상황이 이렇게 되면... 갑자기 차후 계획에 대해서 상상해본 것이 있는데

만약 계획대로 된다면 부스트캠프가 조금 더 재밌어지겠는데 싶은 아이디어가 생겼다.

이제부터 타당성 검토 시간의 시작이다.

차기 계획

1. 계획 타당성 검토

2. ResNet 구현 준비

3. 심화과제 마무리 짓기

</div>
</details>

