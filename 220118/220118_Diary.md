Writing Specification
>Date : 22.01.18
>
>강좌 분류 : boostcamp AI Tech - AI Mathmatics
>
>>강좌 번호 : 4
>>
>>제목 : 경사하강법 매운 맛
>
>>강좌 번호 : 5
>>
>>제목 : 딥 러닝 학습 방법 이해하기
>
>>강좌 번호 : 6
>>
>>제목 : 확률론 맛보기
>
>강좌 분류 : boostcamp AI Tech - Python
>
>>강좌 번호 : 1-3
>>
>>제목 : 파이썬 코딩 환경
>
>>강좌 번호 : 2-1
>>
>>제목 : Variables
>
>>강좌 번호 : 2-2
>>
>>제목 : Function and Console I/O

**오늘 들은 강의 총평 및 나의 감상**

감사하게도 일기 형식의 리포트에 많은 호응을 보내주셔서 이런 포맷을 당분간 써보면서 조금씩 바꿔야겠다.

파이썬 강의에서 가상환경 설치하는 강의(파이썬 코딩 환경)는 정말 가볍게 지나지만 무엇보다 신경써야하는 부분이다.

특히 운영체제(Operating System, OS)에 따라서 아나콘다 가성환경에서 PIP를 이용해서 설치하더라도 어떤 경우에는 되고 어떤 경우에는 실패한다.

지금 할 이야기는 가장 최근에 연구한 것을 개발하다가 생긴 비하인드 스토리이다.

그 프로젝트는 깃허브에 대충 올려 놓았는데, Reference 등을 명확히 확인하고, 사업단에게 의견을 물어서 Public으로 전환해야할 것 같아서 Private으로 두었다. *(포트폴리오를 위해서라면 올리긴 해야하는데, ㅠ_ㅠ 모르는 척하고 올릴까..)*

그 과정에서 DetectoRS라는 객체 탐지 신경망을 사용하기 위해서 MMdetection이라는 라이브러리를 설치해야 했고 카메라 모듈을 설치하기 위해서 Azure Kinect DK SDK(Software Development Kit)를 설치해야 했다.

문제는.. 카메라의 공식 SDK가 Windows에서 C++ 기반으로 작성되어 있었고, MMDetection은 Pytorch, Python기반에 Ubuntu에서 작성되어 있었다.
(Windows에서의 작동을 보증하지 않는 상황! -> 대학원생의 야근이 확정되는 당연한 순간! -> 와 야근이다!)

그래도 어떻게 Windows 10에서 모델을 이식, 동작시키는데 성공해서 실제 시스템 시연까지는 성공했다.

문제는 우리 연구실은 잠깐 하는 줄 알고 시연을 위해서 굉장히 비싼 시스템을 충전소에 두었는데, 다른 교수님께서 그걸 계속 운용하실 계획인 것 같았다.

그래서 다시 시스템을 다운그레이드하여 적절한 가격의 시스템으로 구현하려고 새로 시스템을 구매하고 거기에 코드와 프로그램을 재이식했다.

문제는 그 과정에서 C++ 쪽 코드를 디버깅을 위해서 Visual Studio 2022를 깔았는데, 하던대로 전부 가상환경을 구성했는데 시스템이 동작하지 않아 난처했던 적이 있었다.

수많은 고민 끝에, 원인을 찾았는데, MMDetection 라이브러리를 설치하는 과정에서 **일부 필수 라이브러리가 컴파일러를 따져서, Visual Studio 2015/2019가 아니면 설치가 안 되는 것이 원인** 이었다.

아는 사람은 알겠지만, Ubuntu 환경에서 이런 라이브러리 설치 과정에서 C 계열의 컴파일러를 사용해야하면, 운영체제(또는 가상환경)에 설치된 gcc 같은 컴파일러를 활용하여 설치한다.

보통 저런건 sudo apt upgrade 뭐 이런거 써서 업그레이드해두면 안 되는 것도 잘 되어서(이렇게 말하면 안 되지만) 신경 안 쓰는 부분인데,

Windows에서는 Visual Studio의 c++ 컴파일러를 따라가는 것을 몰랐던 것이다.(근데 이 글을 쓰다가 다시 스스로를 돌아보면 gcc같은 것을 내가 별도로 설치하지 않았는데 컴파일이 될거라고 생각하려는 것도 웃기긴 함.)

> 이걸 찾다가 나중에 알게 된 사실인데, Ubuntu에서 아나콘다 가상환경을 Export하는 것과 Windows에서 아나콘다 가상환경을 Export하는 것은 꽤 다르다.
> 
> 위 두 과정으로 생성된 yml 파일을 열어보면 Windows에서 생성된 파일은 무려 컴파일러 등과 관련된 기본 정보(?) 같은 것이 따로 기술되어 있다.

아무튼, 컴퓨터에 대해 잘 알면 잘 알수록 이런 예기치 못한 상황을 회파하는 것도 가능해진다는 사실을 알 수 있었다.

> 그 이야기 외 강의 내용에서는, 개인적으로 Collab은 뭔가 안 쓰게 된다. *개인 GPU를 사도록 하자.(?)*
> 
> 거기에 난 Jupyter Notebook보단 Pycharm이나 VS code를 쓰고, 심지어 Terminal로 Print하면서 디버깅하는 굉장한 코딩 초보다.

오늘은 왠지 모르게 파이썬 강의에 힘 좀 썼다.(를 빙자한 투-머치 한풀이 타임)

그래서 오늘은 확률론에 조금 더 힘을 주어 공부해볼까 한다.

그 전에, 딥러닝 학습 방법과 역전파를 조금 정리해보자.

> **역전파(Back Propagation)의 순한 맛과 매운 맛, 딥 러닝의 학습 방법**

 딥 러닝, 나아가 기계 학습 이론은 (물론 당연하게도) 수학이 베이스이다.
 
 **아~~ 그럼 무슨 수학을 잘하면 되는 건가요?** 라고 묻는다면.. *그걸 논하기엔 난 공부가 부족한 것 같다.*
 
 하지만, 그럼에도!!! 기계 학습을 위해서 내가 인상 깊게 들었던(=들어야 했던) 강의가 있다면,

> 1. 확률과 통계(Probability Theory and Statistics)
>
> 2. 선형대수학(Linear Algebra)
>
> 3. 최적화 이론(Optimization Theory)

 이 챕터에서는 주로 3번에 관한 이야기를 할 것 같다.

> 그런데 저렇게 써놓고 보니 좀 그렇다.. 어제와 오늘 이 2일 동안 위 강의들을 거의 맛을 본 것이다.(살인적인 부스트캠프의 강의 속도를 체감해버림..)
 
> 더 따지면 수치 해석(Numerical Analysis) 정도가 있겠는데, 난 대학교, 대학원에서 이 강의가 "우리 과"에서 열리는 것을 본 적이 없어 말할 수 있는 것이 없다...
>
> 근데 수치 해석 이야기를 왜 하냐면 역전파가 없다면 수치 해석적으로 신경망을 학습하는 것도 가능하기 때문이다.(교재에 나온 극한식과 h->0으로 보내어 수치해석적으로 편미분을 계산하는 방법이다.)
>
> 대신 이 경우에 데이터 하나당 학습 과정에서 순전파(Forward Propagation)를 2번 해야하는 단점이 있어서 쓰지 않는다고 배웠다. 이러고 그냥 넘어가서 정확히 이걸로 업데이트하는 것이 기억이 안 난다. 가르치시던 교수님이 실제로 구현했던 건데 진짜 드럽게 느리다..
 
 느려 터진 수치해석을 이용한 가중치 업데이트를 제치고 등장한 역전파(Backward Propagation)의 핵심은 미분의 Chain Rule을 활용하는 것으로, Chain Rule을 이용하여 아무리 깊은 신경망을 구성해도 입출력 값과 전파되어 들어온 값을 저장해두면 적당히 활용하여 정해진 미분 폼에 넣어 미분 값을 반환할 수 있다.
 
 자, 그럼 미분을 지금 왜 구하는 걸까?
 
 그 전에 강의에서 나온 목적식(Objective Function, Cost Function, Loss Function)이 무엇을 의미하는지 생각해보고자 한다.
 
 (이거 근데 위의 3개의 단어를 구분하여 사용하는 사람들도 있는 것 같다. 뭐가 정확히 맞는지는 모르겠는데 난 구분하진 않고 쓴다.)
 
 나는 목적 함수라고 배웠는데, 내 식대로 *쉽(지않)게* 설명하면,
 
 신경망의 출력이 목적 함수를 통해 원하는 출력에 근사할 수 있게끔 만들어주는 함수이고, 다른 것을 강조하자면
 
 **신경망의 현재 출력과 원하는 출력 사이의 거리 또는 차이 등을 수학적인 방법으로 평가하고 이를 바탕으로 가중치를 업데이트 하는데 반영되는 함수** 라고 생각한다.
 
> 사실 이건 많은 비평과 고민이 필요한 생각이기도 하다. 혹시 이 문장도 보셨고, 이에 대한 생각이 많이 다르다면 알려주시면 너무너무너무 감사하겠습니다.
 
 이 목적 함수를 기반으로 가중치 업데이트를 하다보니, 원하는 출력과 지금의 출력 사이의 거리를 좁혀야하는 방향으로 가중치가 학습이 되면 좋겠어서 사용하는 것이 경사하강법(Gradient Descent)이다.
 
 미분은 현재 가중치에서 업데이트했을 때 같은 목적 함수로 다시 계산했을 때 그 값이 감소하는 방향을 찾는 수단으로, 목적 함수가 거리, 차이를 나타내는 함수이다보니 이 함수의 출력 크기를 0에 가깝게 보내는 것이 목표인 것이다.
 
 그리고, 이런 방식이 이론적으로 경사하강법으로 학습하는데 성공하려면 이 가중치를 포함한 목적 함수가 하나의 조건을 만족해야한다.
 
 바로 목적 함수가 볼록 함수(Convex Function)이어야 한다는 점이다.
 
 > 참고로 2차원 공간에서 y축을 따라 양의 방향으로 바라보았을 때 볼록하다는 이야기이다.
 >
 > 반대의 경우인 y축을 따라 양의 방향으로 바라보았을 때 오목한 함수는 오목 함수(Concave Function)이라고 한다.
 >
 > 오목 함수를 목적 함수로도 쓸 수 있다고 하는데, 함수에 마이너스를 곱해서 볼록 함수로 바꿀 수 있기 때문인 듯? 근데 그렇게 쓰는 걸 본적이 없다. 아니면 까먹었나... 뭔가 GAN이 min-max game을 하고 있으니 그런 느낌이긴 하다. 이건 한 번 따져봐야할듯?
 
 최적화 이론은 이 볼록 함수에 대해서 배우고 볼록 함수의 최소점을 찾는 과정을 배웠던...것으로 기억하는데 다시 봐야한다. 흑흑...
 
 아무튼 강의에도 언급된 볼록 함수의 정의는 아래와 같다.
 
 볼록 함수 : 정의역 X가 실수 벡터 공간의 볼록 부분 집합(Convex subset, 걍 직역한건데 뭐라고 하는지 모르겠음..)이고, 함수 f는 이 정의역 X를 실수 공간을 치역으로 갖는 함수일 때,
 
 **필요충분조건**으로서 아래의 조건이 성립하는 함수 f를 볼록함수라고 한다.
 
 $f(tx_1+(1-t)x_2) <= tf(x_1)+(1-t)f(x_2) , for all 0<=t<=1 and x_1, x_2 \in X$ 
 
 [출처 - Convex Fucntion, Wikipedia](https://en.wikipedia.org/wiki/Convex_function)
 
 저 함수 식이 무엇을 의미하는가? 이걸 가장 쉽게 이해하는 것은 출처에도 나와있듯이 기하학적으로 생각해보는 것이다.
 
 Convex Subset은, 집합 내 임의로 2개의 원소를 선정해서 그 2개의 원소를 연결하는 직선을 정의하면, 그 직선 내 모든 점들 또한 집합 내 원소일 때 Convex Subset이라고 한다.
 
> 아주 쉽고 어처구니 없이 표현하면 어디 맞아서 찌그러진 것이나 뚫린 것 같지만 않으면 된다.
>
> 참고로 Convex Subset이 아닌 Subset을 Convex Subset이 되도록 최소한으로 크기를 키운 subset을 Convex Hull이라고 함.
>
>수학적으로 표현 시 Convex Subset이 아닌 Subset을 포함하는 가장 작은 크기의 Convex Set을 지칭하는 것이다.
>
>[출처 - Convex Hull](https://en.wikipedia.org/wiki/Convex_hull)
 
 그 Convex Subset 내 원소 2개를 임의로 뽑아 직선으로 연결했을 때 **직선에 포함되는 모든 원소를 뽑아 함수 f에 입력으로 주어 얻은 출력** 을 표현한 것이 위의 식의 **좌변**이다. 
 
 위의 식의 **우변**은 아까 뽑은 원소 2개를 가지고 함수에 입력으로 주어 얻은 **함수값을 실수 공간 상에서의 벡터로 표현한 다음, 두 벡터의 종단점을 이었을 때 얻는 직선**을 표현한 것이다. 
 
> 헷갈리면 출처로 들어가 그림을 보고 이해가 되었으면 좋겠습니다....
 
 최대한 한 문장으로 정리하자면, Convex Subset인 정의역 내 두 점 $x_1, x_2$을 뽑아서 그 함수값 $f(x_1), f(x_2)$를 실수 공간상에서 이은 직선과, 그 정의역 사이에 있는 함수값을 이은 선을 비교해서, 모든 구간에서 직선이 함수값의 선보다 크면 함수 f는 Convex Function이다.
 
 이 함수는 Local minima가 없이 Global minima 하나만 있기 때문에, 경사하강법을 적절한 하이퍼파라미터를 사용하여 반복하면 언젠간 Global minima에 도달할 수 있다는 것이다.
 
> **문제는 가중치를 포함한 목적 함수가 Convex Function이 아닌 경우가 더욱 많다는 점이다.**
> 
> Local minima가 존재하는 등의 이유가 존재하기 때문에 Convex Function이 아님.
 
 앞으로의 기계학습은 이 Local minima를 어떻게 해야 벗어나서 Global minima에 도달할 수 있도록 목적함수를 학습시키는지를 배우는게 기존까지의 강의 흐름으로 기억한다.
 
 확률적 경사 하강법(Stochastic Gradient Descent), mini-batch 역시 메모리 부족 문제를 해결하기 위한 방안이자 이런 식으로 local-minima에 수렴하여(난 이걸 수렁에 빠진다고 표현하는데.) 학습이 실패하는 것을 방지하는 트릭? 기술로써 활용되는 것이다.
 
 이것 말고도,
 
> Optimizer(Adam, RMSprop, and so on.)
>
> Learning Scheduler(Early Stopping, and so on.)
>
> Regularization(Dropout, L1(2)-Regulrization, Batch-Normalization, and so on.)

 등등을 활용하여 학습이 최대한 Global Minima로 도달할 수 있도록 하는 기술, 스킬들을 배울텐데,
 
 **이번 기회에 나 역시 이를 제대로 배우고 직접 활용할 수 있는 지혜와 능력을 갖기를 바란다.**
 


> **확률론 찍어서 먹지 말고 부어서... 통계론이랑 같이 내일 먹어보자**
 
> 너무 앞에서 풀 에너지로 갔나... 거기에 생각해보니 연구실에 내가 공부한 서적들이 가득해서 참고하면 좋겠는데,
>
> 연구실이 있는 미래관이 지금 코로나 집단 감염 사태가 터져서 폐쇄당했다.
>
> *인생이 억까 당하는 중.. ㅠㅠ*
> 
> 안타깝지만 더 나은 정리를 위해 이번 확률론 정리는 내일 해보도록 하겠다.


**내일 스케줄**
1. 기본 과제 2번 풀기
2. AI Math 통계론까지 공부, 퀴즈 풀기
3. 심화과제 1번 Gradient Decent, 3번 Maximum Likelihood Estimation 구현하기.
