<details>
<summary>Writing Specification</summary>
<div markdown="1">

>Date : 22.02.11
>
>강좌 분류 : None
>
>>강좌 번호 : None
>>
>>제목 : None

</div>
</details>

<details>
<summary>나의 주간 강의 총평</summary>
<div markdown="1">

사실.. Data Visualization도 당연히 중요한 내용인데,(난 오히려 이게 한 50%는 먹는다고 보는데)

여기 boostcamp를 진행 중인 모든 캠퍼들에게 너무나도 큰, 말 그대로 **초대형 주제** 중 하나인

Deep Learning Basic이 올라오면서 제대로 묻혀버렸다. 지못미...는 모르겠고 보긴 해야지 ㅎ.

DL Basic은 얼마나 깊이 공부하느냐에 따라서 차후 SOTA 논문을 이해하는 것에 있어서 중요한 밑바탕이 된다.

당연한 소리겠지만, 이렇게 이야기하는 것에는 DL Basic을 제대로 이해해야한다는 중요성을 강조하는 것이다.

그러나 강의를 들으면 알겠지만, DL Basic의 바탕은 수학/공학/통계학 등 모두 기존에 우리가 수강한 과목들이다.

나는 이 부분을 수강하면서 최대한 기존에 있었던 많은 공학적 테크닉에 연관지어서 듣고자 했다.

<details>
<summary>맨날 말하는 뻘한 소리들과 더 나아가기 위한 헛소리들.. 안 봐도 됨.</summary>
<div markdown="1">

내가 이전에 작성했던 RNN에서 나온 FIR/IIR 개념, 왜 중요했는지 기억하는가?

이 개념을 제대로 이해하고 있다면,(최소한 z-domain에서의 전달 함수 개념을 어렴풋이라도 이해한다면)

LSTM에서 gate들이 Activation 함수로 Sigmoid 함수를 써야했던 이유와,

Gate 외에 다른 Signal path들이 ACtivation 함수로 ReLU가 아닌 tanh을 써야했던 이유를 유추할 수 있다.

GRU는 왜 LSTM과 달리 gate를 1개만 사용해서 조절할 수 있었을까?

과거의 정보와 현재의 정보 사이의 조절 비율을 2개의 gate로 각각 조절하는 LSTM과 달리,

1개의 gate와 Bilinear Interpolation 개념을 활용하여 전체 정보의 총합을 1로 만든다는 철학을 이해한다면

GRU가 Gate를 1개만 사용할 수 있다는 점을 이해하기 쉬웠지 않았을까?
>(사실 이건 대학원에서도 이해 못 했었는데)

그리고 그로 인해 얻은 장점은 시스템의 단순화(가중치의 감소)를 유도해내어 학습이 더 잘 된다거나...

이런 철학들을 이해하는게 핵심이라고 생각하고 주력해왔다.(정리는 많아지고, 늦어지고, 강박은 커지지만.)

이렇게 공부하면서 뭐 이런 저런 생각들이 조금 든 것들이 있는데 여기 적고 나중에 한 번 봐야겠다.

1. FCN/CNN/RNN 등 신경망의 철학은 무엇일까? 기존의 시스템에서 어떻게 인사이트를 얻을 수 있을까?

Affine Transformation, 나는 Computer Vision을 대학교에서 수강하면서 이것만큼 많이 들어본 단어가 드물다.

언급은 안해도, 모든 Convolution 필터 개념들이 Affine Transformation 개념을 가지고 있다.
> 내가 알기론 대신 비선형함수를 적용하면 안 된다. 선형 변환이기 때문이다.
>> 이것도 보면, Transformer 생각도 조금 고쳐야한다니까.

Affine Transformation에 대한 생각을 적어놓는 이유는 3번과 직결된다.

2. ResNet의 철학은 무엇일까? 어떤 시스템에 비유할 수 있을까?

ResNet에서의 Shortcut Connection을 시스템 제어적인 측면에서는 바라볼 수는 없을까?

잔차를 조절한다는 개념은 Regulator였나.. 그런 LQR 제어(칼만 필터) 같은 시스템에서 많이 본 것 같다.

3. Transformer가 왜 RNN 같은 Autoregressive Model을 압도할 수 있었을까?

Transformer는 RNN과 달리 모든 데이터를 담아둔다.

나는 여기서 Reinforcement Learning과 Dynamic Programming과의 관계가

RNN 계열 모델과 Transformer 계열 모델의 관계와 유사하다고 생각한다.

모든 데이터에 대한 관계를 Attention으로 규정하고 다차원으로 미리 학습하는 Transformer가

그 모든 관계를 1차원적인 관계로만 학습했던 Autoregressive Model을 압도하는 것이 당연하다면,

Autoregressive Model이 Transformer에서 다루는 모든 변수들의 관계를 다차원으로 학습하게 된다면 

Transformer를 압도할 수 있는 구도가 형성될 수도 있지 않을까?

그리고 그것이 가능하다면, Transformer는 RNN과 다른 것이 무엇일까?
>그러나 지금까지의 나의 의견은 그 둘은 분명히 다른 구조라고 생각한다.
>> 이유는 Positional Encoding이 RNN에 적용할 수 없기 때문이라고(적용하면 RNN이 아니게 되어버리는)
>>
>> 생각 중이긴 한데, 이것도 달라질 순 있다. 늘 그렇듯 정답은 없고 현재까지의 나의 답만 있을 뿐이다.

뭐 그런 생각들? 공부하면서 생각난 것들을 조금 끄적여보았다.

</div>
</details>

뭐... 지켜진 것도, 지키지 못한 것도 있으니 아쉽기는 매한가지이나,

이렇게 공부를 터프하게 해본 적도 대학교 3~4학년 이후로 오랜만이다.

**대학원 때는 뭐했냐고..?** 그 땐 새로운 걸 배우고 해보는 것에 집중하느라,

또 시스템을 구현한다는 재미에 너무 빠져있던 터라, 이렇게 깊은 생각을 할 여유가 없었다.

무엇보다 지도교수님이 부탁하신 일이나 나의 연구를 처리하고 진행하는 것에 최대한 정성들여 집중하다보니

이론적인 공부에 상상을 더할 여유가 없었다는 핑계도 있긴 했고.

취준도 해야하는데, 공부할수록 너무 재밌어진다.

이 괴리가 나한텐 항상 너무 아쉽다.

</div>
</details>

<details>
<summary>나의 주간 총평</summary>
<div markdown="1">

1. 잘했던 것, 좋았던 것, 계속할 것
   
   Transformer를 드디어 이해해보기 시작한 것.
   > Attention is all you need를 다운만 한 6번 받은 것 같다.
   >
   > 그런데 제대로 공부한 적이 없다. 사실 옆 사람이 알려줬는데 이해를 못 했다.
   >
   > 그러나 이번 boostcamp의 강의를 바탕으로 다시 한 번 볼 수 있어서 좋았다.
   >
   > 원래 논문을 먼저 읽어서 나의 insight를 만든 다음에 타인의 insight와 비교하는 것으로 공부하는데
   >
   > 상황이 상황이니만큼, 일단 타인의 insight를 바탕으로 나의 insight를 만드는 것으로 선회하였따.

   AlexNet을 직접 구현해봤다.
   > R-CNN을 공부하는 과정에서 AlexNet을 직접 구현해봤다.
   >
   > 아직 코딩이 미숙하지만, 이렇게 뚝딱뚝딱 해보면서 늘 것이라고 생각하고 있다.
   >
   > 추가적으로 ViT를 구현해보고 싶어졌다. 갑자기 개인적으로 궁금해진 것이 생겨서.
   >> Positional Encoding이 이미지에서 어떻게 영향을 미치는지 실험적으로 궁금해졌다.

   RCNN을 공부(해보긴)했다.
   > ~~단점이 더 많을 듯 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ~~

   ResNet 구현한 것을 A/S해보고 있다.
   > Github에 구현한 것을 당당히 post할 수 있는 수준으로 구현하고자 함.
   >
   > Tensorboard 등을 활용해보려고 함.

2. 잘못했던 것, 아쉬운 것, 부족한 것과 그 개선 방향
   
   R-CNN 공부/구현이 제대로 안 된 것.**(멘탈 나감 주의)**
   > 일단 R-CNN은 그냥 하기엔 너무 어려운 주제였다.(당연하다.)
   >
   > 그러나 그 것이 면죄부가 될 순 없다.(이것도 당연하다.)
   >> 심지어 공부가 제대로 안 된 증거로 내용 파악을 잘못하고 있었음. 이건 면죄가 안 됨. ㄹㅇㅋㅋ
   >
   > 고로 난 ~~멍청이~~ 다 ㅠㅠ.(이것도 당연하....면 안 되는데?)
   >> 멍청이는 공부를 해야한다. ~~커피를 사와라 멍청이. 밤샘이다. ~~

   생활 패턴이 망가졌다.
   > 심각하게 망가졌다. 11시간자고 4시간자고 4시간자고 11시간자고 4시간자고 2시간자고 11시간자고 ~~레전드~~
   >
   > 운동도 안한다. 극약처방으로 일단 먹는 것을 줄이고 있긴 한데.. 운동을 해야한다.
   >
   > 근육이 뻐근해지고 손목이 아파오고 있다. 리얼 위험한 징조다.
   >> 따라서, 스스로 뭔가 바뀌긴 해야한다.

3. 도전할 것, 시도할 것
   
   도전할 것 : Transformer 공부 및 발표 준비

   시도할 것 : ViT 공부 및 구현, Positional Encoding에 따른 변화 실험

   둘 다 할 수 있도록 최선을 다해야겠다.


4. 키워드, 알게된 것, 느낀 점
   
   AlexNet을 구현해보았다.

   기존의 지식과 더불어 Transformer를 이해해보려고 했다.

</div>
</details>

<details>
<summary>후일담 및 차기 계획</summary>
<div markdown="1">

주말은 늘 바쁘다.

> 모든 주말은 바쁘다.

그런데 주중도 늘 바쁘다.

> 모든 주중은 바쁘다.

주중과 주말을 합치면 매일이 된다.

> 매일이..바쁘다?

~~ ??? : 그만해~~~~ 이러다 다 죽어~~~~~~~~

주말 간 계획

1. Transformer 공부 및 발표 준비
   
2. 이번 주 정리 미흡한 부분 수정 및 Notion 개제
   
3. 스몰톡 준비해보기

</div>
</details>

